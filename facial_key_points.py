# -*- coding: utf-8 -*-
"""facial key-points.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yKmKbK6HgHjGjQu5miaxpaxVrfxYjPqz
"""

'''
The objective:
predict keypoint positions on face images.
'''


#step 0: loading data into the colab
#Opt 1: Import data stroage from google
from google.colab import drive
drive.mount('/content/drive')

#step 1: initialize the running enviroment
from __future__ import print_function
import keras
import gc
import tensorflow as tf
import torch
import pandas as pd
from keras.callbacks import EarlyStopping
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, BatchNormalization
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
import pandas as pd
import numpy as np
from scipy.stats import mode
import matplotlib.pyplot as plt
from keras.applications.vgg16 import VGG16
from keras.applications.densenet import DenseNet121
from keras_preprocessing.image import ImageDataGenerator
from keras import backend as K
from IPython.display import clear_output
from time import sleep
import os

#init Parameters

#step 2: Load Training data to memory 
'''
In train_data:
totalImage=7049
totalLabel=31
'''
test_dir = '/content/drive/My Drive/Colab Notebooks/Kaggle/facial key-points/DATA/test.csv'
train_dir = '/content/drive/My Drive/Colab Notebooks/Kaggle/facial key-points/DATA/training.csv'
lookid_dir = '/content/drive/My Drive/Colab Notebooks/Kaggle/facial key-points/DATA/IdLookupTable.csv'

test_data=pd.read_csv(test_dir)
train_data=pd.read_csv(train_dir)
lookid_data=pd.read_csv(lookid_dir)
os.listdir('/content/drive/My Drive/Colab Notebooks/Kaggle/facial key-points/DATA')

'''
value_counts:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html
isnull.any():https://chartio.com/resources/tutorials/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe/
fillna:https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html
Pandas head() method is used to return top n (5 by default) rows of a data frame or series; .T is equavlent to transpose() function
'''
#train_data.head(n=8).T
print(train_data)
#train_data.isnull().any().value_counts() #28 ture, 3 False in columns
train_data.fillna(method= 'ffill', inplace=True)
#train_data.isnull().any().value_counts() #31 ture, 0 False in columns


#Training Data
'''
split():https://www.w3schools.com/python/ref_string_split.asp
'''
imag = []
for i in range(0,7049):
    img = train_data['Image'][i].split(' ')    #spliting data into lists
    img = ['0' if x == '' else x for x in img] #replacing missing value to 0
    imag.append(img)

image_data=np.array(imag).astype('float32')
X_train=torch.Tensor(image_data).reshape(-1,96,96,1).numpy().astype('float32')/255
del image_data

#the block above takes too long to compile, thus i restarted a block here
'''
matplotlib.pyplot.imshow():https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html
'''
#print(X_train)
plt.imshow(X_train[0].reshape(96,96),cmap='gray')
plt.show()

#split the labels
'''
pandas.dataframe.drop():https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html
":": this is called sliceing 
pandas.dataframe.iloc():https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html 
'''
#print(X_train)
trainTemp=train_data.drop('Image',axis=1)
Y_train = []
for i in range(0,7049):
  y = trainTemp.iloc[i,:] # i row all column, which means that all data on row i
  Y_train.append(y) 

Y_train=np.array(Y_train).astype('float32')
print(Y_train)
del trainTemp
#gc.collect()

# step 3: Time to build the model

'''
trans = keras.layers.Conv2D(filters=3,kernel_size=5,padding='same')
des = DenseNet121(include_top=True, weights=None,input_shape=(96,96,1), classes=30)
busted model: 
#add components to model
model = Sequential()
model.add(trans)
model.add(des)
#model.add(Dense(512,activation='relu'))
#model.add(Dropout(0.1))
#model.add(Dense(30,activation='softmax')
#model.add(firstLayer(input_shape=(96,96,1))
'''


conv2d_1 =  Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (96,96,1))
conv2d_2 =  Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu')
pool_1 =    MaxPooling2D(pool_size=(2,2))
drop_1 =    Dropout(0.1)


conv2d_3 =  Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
conv2d_4 =  Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
pool_2 =    MaxPooling2D(pool_size=(2,2),strides = (2,2))
drop_2 =    Dropout(0.1)



conv2d_5 =  Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
conv2d_6 =  Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
pool_3 =    MaxPooling2D(pool_size=(2,2),strides = (2,2))
drop_3 =    Dropout(0.1)



conv2d_7 =  Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
conv2d_8 =  Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
pool_4 =    MaxPooling2D(pool_size=(2,2),strides = (2,2))
drop_4 =    Dropout(0.1)



conv2d_9 =  Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
conv2d_10 =  Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
pool_5 =    MaxPooling2D(pool_size=(2,2),strides = (2,2))
drop_5 =    Dropout(0.1)


conv2d_11 =  Conv2D(filters = 512, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
conv2d_12 =  Conv2D(filters = 512, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu')
pool_6 =    MaxPooling2D(pool_size=(2,2),strides = (2,2))
drop_6 =    Dropout(0.1)

drop_7 =    Dropout(0.25)


model = Sequential()

model.add(conv2d_1)
model.add(conv2d_2)
model.add(BatchNormalization())
model.add(pool_1)
model.add(drop_1)

model.add(conv2d_3)
model.add(conv2d_4)
model.add(BatchNormalization())
model.add(pool_2)
model.add(drop_2)

model.add(conv2d_5)
model.add(conv2d_6)
model.add(BatchNormalization())
model.add(pool_3)
model.add(drop_3)

model.add(conv2d_7)
model.add(conv2d_8)
model.add(BatchNormalization())
model.add(pool_4)
model.add(drop_4)

model.add(conv2d_9)
model.add(conv2d_10)
model.add(BatchNormalization())
model.add(pool_5)
model.add(drop_5)

model.add(conv2d_11)
model.add(conv2d_12)
model.add(BatchNormalization())
model.add(pool_6)
model.add(drop_6)

model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(drop_7)
model.add(Dense(30, activation='relu'))

model.compile(loss='mean_squared_error',
        optimizer=keras.optimizers.Adam(lr=1e-3),
        metrics=['mse'])

### TRAINING START! ###
model.fit(X_train,Y_train,epochs = 50,batch_size = 256, validation_split = 0.2)
#gc.collect()

#test_data.isnull().any().value_counts()
test_data.dropna
test_data.isnull().any()

### TESTING START! ###
#Testing Data1
print(test_data)
timag = []
for j in range(0,1783):
    timg = test_data['Image'][j].split(' ')    #spliting data into lists
    #timg = ['0' if x == '' else x for x in timg]#replacing missing value to 0
    for i in range(0,30):
      temp = timg[i]
      if temp == '':
        del timg
      del temp
    timag.append(timg)

timage_data=np.array(timag).astype('float32')
X_test=torch.Tensor(timage_data).reshape(-1,96,96,1).numpy().astype('float32')/255
print(X_test.shape)
del timage_data

plt.imshow(X_test[0].reshape(96,96),cmap='gray')
plt.show()

#TEST Opt#2

pred = model.predict(X_test)
#pred = np.argmax(pred, axis=1)

'''
print(lookid_data)
lookid_submission=lookid_data.drop('Location',axis=1)
#print(lookid_submission)
#print(pred)


pred_array=np.array(pred)
print(pred_array.shape)
pred_len=len(pred_array)
pred_ready=[]
pred_temp=[]
#print(pred_array)

#print(lookid_data)
pred_1d=pred.flatten()
#print(pred_1d)
for i in range(len(pred_1d)):
  pred_temp=pred_1d[i]
  pred_ready.append(pred_temp)
pred_submission=pd.DataFrame(data = pred_ready, columns=['Location'] )
print(pred_submission)

'''
##############################################################################################################
print(lookid_data)
lookid_list = list(lookid_data['FeatureName'])
lookid_list = list(pd.Series(lookid_list).unique())
#print('lookid_list: ', lookid_list)

imageID = list(lookid_data['ImageId']-1)
print('imageID: ', imageID)
pre_list = list(pred)
#print(pred)
#print(pre_list)
rowid = lookid_data['RowId']
rowid=list(rowid)
#print(rowid)

# filter out unrequired feature names and map the existing features with numbers
feature = []
for f in list(lookid_data['FeatureName']):
    #print('f= ',f)
    feature.append(lookid_list.index(f))
#print('feature: ', feature)

# extract required results with corrospoding feature index and image id, from the tested result
preded = []
for x,y in zip(imageID,feature):
    preded.append(pre_list[x][y])
#preded=pd.DataFrame(data=np.array(preded), columns = ['Location'])
#print(preded)

subm = lookid_data.drop(['ImageId', 'FeatureName','Location'], axis=1)
#print(subm) 

submission = pd.DataFrame({'RowId': subm['RowId'],'Location':preded})
submission.to_csv('face_key_detection_submission.csv',index = False)
'''
rowid = pd.Series(rowid,name = 'RowId')

loc = pd.Series(preded,name = 'Location')

submission = pd.concat([rowid,loc],axis = 1)
print(submission)

submission.to_csv('face_key_detection_submission.csv',index = False)
'''